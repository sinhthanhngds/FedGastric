{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393012e1",
   "metadata": {},
   "source": [
    "# iLab Project: Group 7-2\n",
    "\n",
    "#### Federated Learning for Postoperative Gastric Cancer Detection\n",
    "\n",
    "David Bain 91082596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7c474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install torch-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d203477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.4.0-cp311-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached torch-2.4.0-cp311-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a92b3e76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T09:00:35.901805Z",
     "start_time": "2024-08-29T09:00:35.898785Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BotoCoreError, ClientError\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mv2\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchvision/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchvision/models/__init__.py:31\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The models subpackage contains definitions for the following model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03marchitectures:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m.. _ResNet: https://arxiv.org/abs/1512.03385\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malexnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchvision/models/alexnet.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_zoo\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmodel_zoo\u001b[39;00m\n\u001b[1;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlexNet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malexnet\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import io\n",
    "\n",
    "import boto3\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from typing import Optional\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as v2\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc18c36",
   "metadata": {},
   "source": [
    "Read in the aws keys to access s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bd147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:19:44.924629Z",
     "start_time": "2024-08-29T08:19:44.921134Z"
    }
   },
   "outputs": [],
   "source": [
    "key_df = pd.read_csv('../team_user_accessKeys.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c168c8",
   "metadata": {},
   "source": [
    "Access the s3 bucket with the images and create an s3_client object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53de7aea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:19:45.503819Z",
     "start_time": "2024-08-29T08:19:44.932461Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace the following values with your access key details\n",
    "AWS_ACCESS_KEY_ID = key_df.iloc[0,0]\n",
    "AWS_SECRET_ACCESS_KEY = key_df.iloc[0,1]\n",
    "region_name = 'ap-southeast-2'\n",
    "\n",
    "# Create an S3 client using the provided access keys\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "# List all the buckets\n",
    "buckets = s3_client.list_buckets()\n",
    "for bucket in buckets['Buckets']:\n",
    "    print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92166c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:19:45.521808Z",
     "start_time": "2024-08-29T08:19:45.519909Z"
    }
   },
   "outputs": [],
   "source": [
    "bucket_name = 'gastric-cancer-data'\n",
    "clinic_0_train_data = [\n",
    "    '0/train_data/0/',\n",
    "    '0/train_data/1/'\n",
    "]\n",
    "clinic_0_test_data = [\n",
    "    '0/test_data/0/',\n",
    "    '0/test_data/1/'    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c2379",
   "metadata": {},
   "source": [
    "[1] Load data from clinic 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbbaabb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:19:45.532399Z",
     "start_time": "2024-08-29T08:19:45.529983Z"
    }
   },
   "outputs": [],
   "source": [
    "def list_s3_bucket_images(s3_client: 'boto3.client', bucket_n: str, prefix: str = '') -> List[str]:\n",
    "    \n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_n, Prefix=prefix)\n",
    "\n",
    "    image_keys = []\n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                if obj['Key'].lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_keys.append(obj['Key'])\n",
    "    return image_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cccdabc",
   "metadata": {},
   "source": [
    "[2] Read an image from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91172f93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:19:45.541303Z",
     "start_time": "2024-08-29T08:19:45.538886Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_image_from_s3(s3_client: 'boto3.client', bucket_n: str, image_key: str) -> Optional[Image]:\n",
    "\n",
    "    try:\n",
    "        obj = s3_client.get_object(Bucket=bucket_n, Key=image_key)\n",
    "        img_data = obj['Body'].read()\n",
    "\n",
    "        # Convert bytes data to a file-like object\n",
    "        img_bytes = io.BytesIO(img_data)\n",
    "\n",
    "        # Use PIL to open the image\n",
    "        image = Image.open(img_bytes).convert('RGB')\n",
    "\n",
    "        return image\n",
    "\n",
    "    # Handle S3 related errors\n",
    "    except (BotoCoreError, ClientError) as e:\n",
    "        print(f\"Error accessing S3 for image {image_key}: {e}\")\n",
    "\n",
    "    # Handle decoding errors\n",
    "    except (ValueError, IOError) as e:\n",
    "        print(f\"Error decoding image {image_key}: {e}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e09fe73",
   "metadata": {},
   "source": [
    "\n",
    "[3] Process the images from disk without transformation.\n",
    "\n",
    "Resizing depends on the desired neural network architecture which have a min of 32 x 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7273c01a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:19:45.548259Z",
     "start_time": "2024-08-29T08:19:45.545827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in all images from disk and store in an array. Preprocessing is done later\n",
    "\n",
    "def load_images(s3_client: 'boto3.client', bucket_n: str, prefixes: List[str]) -> List[Tuple[int, Image]]:\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for prefix in prefixes:\n",
    "        print(f\"Processing images for prefix: {prefix}\")\n",
    "        image_keys = list_s3_bucket_images(s3_client, bucket_n, prefix)\n",
    "        \n",
    "        for image_key in image_keys:\n",
    "            try:\n",
    "                image = read_image_from_s3(s3_client, bucket_n, image_key)\n",
    "                if image is not None:\n",
    "                    bin_image_key = image_key.split('/')[2] \n",
    "                    images.append((int(bin_image_key), image))\n",
    "                     \n",
    "                else:\n",
    "                    print(f\"Failed to load image {image_key}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {image_key}: {e}\")\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd8cb6",
   "metadata": {},
   "source": [
    "Define the s3 'prefix' which relates to the directory structure or clinic: 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d4e01",
   "metadata": {},
   "source": [
    "Define image preprocessiing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc5ac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:19:45.554765Z",
     "start_time": "2024-08-29T08:19:45.552755Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_images(s3_client: 'boto3.client', bucket_name: str, prefixes: List[str]) -> List[Tuple[int, Image]]:\n",
    "    \n",
    "    # List all image keys\n",
    "    for prefix in prefixes:\n",
    "        print(f\"Listing images for prefixes: {prefix}\")\n",
    "        image_keys = list_s3_bucket_images(s3_client, bucket_name, prefix)\n",
    "        print(f\"Total number of images: {len(image_keys)}\\n\")\n",
    "    \n",
    "    # Process images in memory\n",
    "    loaded_images = load_images(s3_client, bucket_name, prefixes)\n",
    "    print(f\"Total number of loaded images: {len(loaded_images)}\")\n",
    "    \n",
    "    return loaded_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f54b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:25:13.436469Z",
     "start_time": "2024-08-29T08:19:45.559418Z"
    }
   },
   "outputs": [],
   "source": [
    "clinic_0_train_images = preprocess_images(s3_client, bucket_name, clinic_0_train_data)\n",
    "clinic_0_test_images = preprocess_images(s3_client, bucket_name, clinic_0_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c2fea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:25:13.450956Z",
     "start_time": "2024-08-29T08:25:13.448671Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print first and last image and classification\n",
    "print(clinic_0_train_images[0])\n",
    "print(clinic_0_train_images[-1])\n",
    "\n",
    "print(clinic_0_test_images[0])\n",
    "print(clinic_0_test_images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a3669c",
   "metadata": {},
   "source": [
    "Define function to print a handful of original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fbb2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:25:13.562210Z",
     "start_time": "2024-08-29T08:25:13.559584Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_images(images: List[Image], titles: List[str], rows: int, cols: int) -> None:\n",
    "    \"\"\"\n",
    "    Plot a list of images with their titles using matplotlib.\n",
    "\n",
    "    :param images: List of images to plot\n",
    "    :param titles: List of titles corresponding to the images\n",
    "    :param rows: Number of rows in the plot\n",
    "    :param cols: Number of columns in the plot\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for img, ax, title in zip(images, axes, titles):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848be43f",
   "metadata": {},
   "source": [
    "Read images and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1a425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:25:13.843878Z",
     "start_time": "2024-08-29T08:25:13.582658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Access a random sample of 5 images\n",
    "if clinic_0_train_images:\n",
    "    random_sample = random.sample(clinic_0_train_images, min(10, len(clinic_0_train_images)))\n",
    "    sampled_images = [img_data for label, img_data in random_sample]\n",
    "    sampled_titles = [label for label, img_data in random_sample]\n",
    "\n",
    "    # Display images using matplotlib\n",
    "    num_images = len(sampled_images)\n",
    "    plot_images(sampled_images, sampled_titles, 2, int(np.ceil(num_images / 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbac359",
   "metadata": {},
   "source": [
    "Resolution is not very good\n",
    "\n",
    "Review image size differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3d2bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:25:13.856424Z",
     "start_time": "2024-08-29T08:25:13.854221Z"
    }
   },
   "outputs": [],
   "source": [
    "sampled_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155ecfe",
   "metadata": {},
   "source": [
    "Resolution of images are small in size and vary. This will limit the transfer learning models used "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c372d",
   "metadata": {},
   "source": [
    "Image Transformation\n",
    "1) Transform to tensor\n",
    "2) Normalise\n",
    "3) Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8dec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:25:13.881879Z",
     "start_time": "2024-08-29T08:25:13.877934Z"
    }
   },
   "outputs": [],
   "source": [
    "def resize_images(images: List[Tuple[int, Image]], size: List[int]) -> List[Tuple[int, Image]]:\n",
    "    \n",
    "    resized_images = []\n",
    "    transform = v2.Resize(size)\n",
    "    \n",
    "    for label, image in images:\n",
    "        resized_images.append((label, transform(image)))\n",
    "    \n",
    "    return resized_images\n",
    "    \n",
    "\n",
    "def convert_to_tensors(images: List[Tuple[int, Image]]) -> List[Tuple[int, Tensor]]:\n",
    "    \n",
    "    tensor_images = []\n",
    "    transform = v2.ToTensor()\n",
    "    for label, image in images:\n",
    "        tensor_images.append((label, transform(image)))\n",
    "\n",
    "    return tensor_images\n",
    "\n",
    "\n",
    "def compute_mean_std(tensor_label_images: List[Tuple[int, Tensor]]) -> Tuple[List[float], List[float]]:\n",
    "    \n",
    "    # Extract tensor images\n",
    "    tensor_images = [image for _, image in tensor_label_images]\n",
    "\n",
    "    # Stack tensors along dimension 0 (creating a new batch dimension)\n",
    "    stacked_tensors = torch.stack(tensor_images, dim=0)\n",
    "\n",
    "    # Compute mean and std along the batch dimension (0), excluding the channel dimension (1)\n",
    "    mean = stacked_tensors.mean(dim=(0, 2, 3))\n",
    "    std = stacked_tensors.std(dim=(0, 2, 3))\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def normalise_images(tensor_label_images: List[Tuple[int, Tensor]], mean: float, std: float):\n",
    "    \n",
    "    transform_normalize = v2.Normalize(mean, std)\n",
    "    normalised_label_images = []\n",
    "\n",
    "    for label, tensor_image in tensor_label_images:\n",
    "        normalized_image = transform_normalize(tensor_image)\n",
    "        normalised_label_images.append((label, normalized_image))\n",
    "\n",
    "    return normalised_label_images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f40ea0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:25:13.889913Z",
     "start_time": "2024-08-29T08:25:13.887679Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_images(target_size: int, images: List[Tuple[int, Image]]) -> List[Tuple[int, Tensor]]:      \n",
    "    \n",
    "    resized_images = resize_images(clinic_0_train_images, target_size)\n",
    "    \n",
    "    tensor_images = convert_to_tensors(resized_images)\n",
    "    \n",
    "    mean, std = compute_mean_std(tensor_images)\n",
    "\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Std Dev:\", std)\n",
    "\n",
    "    normalised_tensor_images = normalise_images(tensor_images, mean, std)\n",
    "    \n",
    "    return normalised_tensor_images\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f3d3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:25:14.270940Z",
     "start_time": "2024-08-29T08:25:13.894716Z"
    }
   },
   "outputs": [],
   "source": [
    "target_size = [32, 32]\n",
    "\n",
    "clinic_0_train_ds = transform_images(target_size, clinic_0_train_images)\n",
    "clinic_0_test_ds = transform_images(target_size, clinic_0_test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7096d7",
   "metadata": {},
   "source": [
    "Create a custom Dataset for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2a0aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:25:14.283372Z",
     "start_time": "2024-08-29T08:25:14.280852Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a custom Dataset for the images\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the item (label, image) from the list\n",
    "        label, image = self.data[idx]\n",
    "        # Return the image and label\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1697f3",
   "metadata": {},
   "source": [
    "Create an instance of the custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3b9a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:25:14.295169Z",
     "start_time": "2024-08-29T08:25:14.293256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create custom datasets\n",
    "\n",
    "train_dataset = CustomDataset(clinic_0_train_ds)\n",
    "test_dataset = CustomDataset(clinic_0_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3d3e1",
   "metadata": {},
   "source": [
    "Create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf9d0d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:25:14.306922Z",
     "start_time": "2024-08-29T08:25:14.304636Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6678acd",
   "metadata": {},
   "source": [
    "Make sure this runs on Mac GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62daf22f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:25:14.325206Z",
     "start_time": "2024-08-29T08:25:14.316531Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5dc545",
   "metadata": {},
   "source": [
    "Define the first Torchvision model: VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e11e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:25:16.021506Z",
     "start_time": "2024-08-29T08:25:14.335076Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models.vgg16(weights=None)\n",
    "\n",
    "# Modify the final layer to suit binary classification\n",
    "num_features = model.classifier[6].in_features\n",
    "\n",
    "# adjust the output layer for binary classification\n",
    "model.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear (25088, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4096, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear (512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear (256, 1),\n",
    "        nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00909a4",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a67139c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:53:04.565555Z",
     "start_time": "2024-08-29T08:51:55.954351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimiser\n",
    "optimiser = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    model.train() # Set model to train mode\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        # Move inputs to labels to the appropriate device and convert to float\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimise\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        # Update running loss\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    \n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e214681d",
   "metadata": {},
   "source": [
    "Test Classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b26f1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:54:31.019765Z",
     "start_time": "2024-08-29T08:54:31.015835Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = 0\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device).float(), labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predicted = torch.sigmoid(outputs).round()\n",
    "            total += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            incorrect_predictions += (predicted != labels).sum().item()\n",
    "\n",
    "    accuracy = correct_predictions / total\n",
    "    print(f\"Correct predictions: {correct_predictions}\")\n",
    "    print(f\"Incorrect predictions: {incorrect_predictions}\")\n",
    "    return total_loss / len(test_loader), accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d3aa22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T08:58:30.388043Z",
     "start_time": "2024-08-29T08:58:29.499715Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the model for accuracy and loss\n",
    "\n",
    "test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984a152",
   "metadata": {},
   "source": [
    "Check for duplicate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b15549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T09:12:58.272873Z",
     "start_time": "2024-08-29T09:12:58.269973Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_for_duplicates(train_loader, test_loader):\n",
    "    train_images_set = set()\n",
    "    test_images_set = set()\n",
    "\n",
    "    # Extract images from the training dataset\n",
    "    for images, _ in train_loader:\n",
    "        for image in images:\n",
    "            # Convert image tensor to a tuple (hashable type for set)\n",
    "            train_images_set.add(tuple(image.numpy().flatten()))\n",
    "\n",
    "    # Extract images from the test dataset\n",
    "    for images, _ in test_loader:\n",
    "        for image in images:\n",
    "            # Convert image tensor to a tuple (hashable type for set)\n",
    "            test_images_set.add(tuple(image.numpy().flatten()))\n",
    "\n",
    "    # Check for duplicates between train and test datasets\n",
    "    duplicates = train_images_set.intersection(test_images_set)\n",
    "    return len(duplicates), duplicates\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dec541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T09:13:31.549857Z",
     "start_time": "2024-08-29T09:13:30.859085Z"
    }
   },
   "outputs": [],
   "source": [
    "num_duplicates, duplicates = check_for_duplicates(train_loader, test_loader)\n",
    "print(f\"Number of duplicates: {num_duplicates}\")\n",
    "if num_duplicates > 0:\n",
    "    print(\"Duplicates found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
