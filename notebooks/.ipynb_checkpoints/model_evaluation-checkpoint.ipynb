{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "486f7af1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T16:25:07.408024Z",
     "iopub.status.busy": "2024-09-22T16:25:07.407159Z",
     "iopub.status.idle": "2024-09-22T16:25:07.413104Z",
     "shell.execute_reply": "2024-09-22T16:25:07.412112Z",
     "shell.execute_reply.started": "2024-09-22T16:25:07.407989Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0129713d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T16:25:07.415758Z",
     "iopub.status.busy": "2024-09-22T16:25:07.415389Z",
     "iopub.status.idle": "2024-09-22T16:25:07.425669Z",
     "shell.execute_reply": "2024-09-22T16:25:07.424749Z",
     "shell.execute_reply.started": "2024-09-22T16:25:07.415727Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "041fd5bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T16:25:07.426981Z",
     "iopub.status.busy": "2024-09-22T16:25:07.426737Z",
     "iopub.status.idle": "2024-09-22T16:25:07.436866Z",
     "shell.execute_reply": "2024-09-22T16:25:07.436035Z",
     "shell.execute_reply.started": "2024-09-22T16:25:07.426960Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Subset, ConcatDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "from torchvision import models, datasets\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e81d7e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T16:25:07.438063Z",
     "iopub.status.busy": "2024-09-22T16:25:07.437830Z",
     "iopub.status.idle": "2024-09-22T16:25:07.446852Z",
     "shell.execute_reply": "2024-09-22T16:25:07.445947Z",
     "shell.execute_reply.started": "2024-09-22T16:25:07.438043Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f14ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0f28c",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c10d4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalData:\n",
    "    def __init__ (self, clinic_id, data_range): #data_range will be removed for final code\n",
    "        self.clinic_id = clinic_id\n",
    "        self.path = f'../120_dataset/{clinic_id}/'\n",
    "        self.range = data_range #remove for final code\n",
    "        \n",
    "        \n",
    "    def dataset (self):\n",
    "        transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  # Resize images to a fixed size (optional)\n",
    "        transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]) \n",
    "        ])\n",
    "\n",
    "# Load the dataset from the train folder\n",
    "        train_dataset = datasets.ImageFolder(root=f'{self.path}', transform=transform)\n",
    "        subset_indices = list(self.range)    #remove for final code\n",
    "        train_dataset = Subset (train_dataset, subset_indices) #remove for final code\n",
    "\n",
    "        train_size = int(0.8*len(train_dataset))\n",
    "        val_size = len(train_dataset)-train_size\n",
    "\n",
    "        train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "        return train_subset, val_subset\n",
    "    \n",
    "    def dataloader(self):  \n",
    "        train_subset, val_subset = self.dataset()\n",
    "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "        print (f'loading {self.clinic_id}')\n",
    "        return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a4fbfd",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2994c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc55f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18 (weights='DEFAULT'): #James\n",
    "    \n",
    "    resnet18 = models.resnet18(weights = weights).to(device)\n",
    "    for param in resnet18.parameters():\n",
    "        param.requires_grad = False\n",
    "    resnet18.fc = nn.Sequential (\n",
    "    nn.Linear(in_features = 512, out_features = 256, bias = True),\n",
    "    nn.Dropout(p = 0.5),\n",
    "    nn.Linear(in_features = 256, out_features = 1, bias = True),\n",
    "    nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "    for param in resnet18.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    resnet18.__class__.__name__ = 'ResNet18'\n",
    "    return resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc1dcf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, drop_out=0.2):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.drop_out = nn.Dropout(drop_out)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            out += self.downsample(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop_out(out)\n",
    "        return out\n",
    "\n",
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, drop_out=0.2):\n",
    "        super(CustomResNet18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Replace residual blocks with custom BasicBlock including dropout\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1, drop_out=drop_out)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2, drop_out=drop_out)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2, drop_out=drop_out)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2, drop_out=drop_out)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, 1000)\n",
    "        \n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride, drop_out):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride, drop_out))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def custom_resnet18(weights='DEFAULT', drop_out=0.2):\n",
    "    custom_resnet18 = CustomResNet18(drop_out=0.2)\n",
    "    custom_resnet18.load_state_dict(models.resnet18(weights = weights).state_dict(), strict=False)\n",
    "\n",
    "# Then freeze parameters\n",
    "    for param in custom_resnet18.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    custom_resnet18.fc = nn.Sequential (\n",
    "    nn.Linear(in_features = 512, out_features =1),\n",
    "    nn.Sigmoid()\n",
    "    )\n",
    "    for param in custom_resnet18.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    return custom_resnet18\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83940930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16(weights = 'DEFAULT'): #David\n",
    "    vgg16 = models.vgg16(weights=weights).to(device)\n",
    "\n",
    "# Freeze the parameters of the base model\n",
    "    for param in vgg16.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "# Modify the classifier part for binary classification\n",
    "    vgg16.classifier[6] = nn.Sequential(\n",
    "        nn.Linear(vgg16.classifier[6].in_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(512, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "    vgg16.__class__.__name__ = 'VGG16'\n",
    "    return vgg16\n",
    "\n",
    "    \n",
    "def vgg19 (weights='DEFAULT'):\n",
    "    vgg19 = models.vgg19 (weights=weights).to(device)\n",
    "    \n",
    "    for param in vgg19.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    vgg19.classifier = nn.Sequential (\n",
    "        nn.Linear(25088, 4096),        \n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(4096, 4096),       \n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(4096, 1),\n",
    "        nn.Sigmoid()          \n",
    "    )\n",
    "    for param in vgg19.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    vgg19.__class__.__name__ = 'VGG19'\n",
    "    return vgg19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df8af5",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c356a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def calculate_accuracy(outputs, labels, threshold=0.5):\n",
    "    preds = (outputs > threshold).float()\n",
    "    correct = (preds == labels).float().sum()\n",
    "    accuracy = correct / labels.size(0)\n",
    "    return accuracy\n",
    "\n",
    "def train_local_model(model, train_loader, val_loader, num_epochs=10):\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        total_train = 0\n",
    "        threshold = 0.5\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).squeeze(1)\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_accuracy += calculate_accuracy(outputs, labels, threshold)\n",
    "            total_train += 1\n",
    "\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_accuracy = 0.0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images).squeeze(1)\n",
    "                loss = criterion(outputs, labels.float())\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += calculate_accuracy(outputs, labels, threshold)\n",
    "                total_val += 1\n",
    "\n",
    "        avg_val_loss = val_loss / total_val\n",
    "        avg_val_accuracy = val_accuracy / total_val\n",
    "        print (f\"Epoch {epoch + 1}/{num_epochs}:\\ntrain_loss: {running_loss / total_train}, train_accuracy: {running_accuracy / total_train}\\nValidation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}\")\n",
    "    \n",
    "    return model, avg_val_loss, avg_val_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe7fe15",
   "metadata": {},
   "source": [
    "### Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e61e4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_averaging (client_weights):\n",
    "    avg_weights = client_weights[0].copy()\n",
    "    \n",
    "    for key in avg_weights.keys():\n",
    "        for key in avg_weights.keys():\n",
    "            for i in range (1, len (client_weights)):\n",
    "                avg_weights[key] += client_weights[i][key]\n",
    "                \n",
    "            avg_weights[key] = avg_weights[key] / len (client_weights)\n",
    "            \n",
    "    return avg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1381a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_learning (model, num_clients, num_rounds, train_loaders, val_loaders):\n",
    "    global_model = model('DEFAULT')\n",
    "    global_weights = global_model.state_dict()\n",
    "    \n",
    "    for round_num in range (num_rounds):\n",
    "        print (f'Round {round_num+1}')\n",
    "        \n",
    "        client_weights = []\n",
    "        \n",
    "        for client_id in range (num_clients):\n",
    "            print (f'client {client_id+1} training...')\n",
    "            \n",
    "            local_model = model(None)\n",
    "            local_model.load_state_dict (global_weights)\n",
    "            local_model.to(device)\n",
    "            \n",
    "            client_train_loader = train_loaders[client_id]\n",
    "            client_val_loader = val_loaders[client_id]\n",
    "            \n",
    "            output_model, _, _ = train_local_model (local_model, client_train_loader, client_val_loader)\n",
    "            client_updated_weights = output_model.state_dict()\n",
    "            \n",
    "            client_weights.append (client_updated_weights)\n",
    "            \n",
    "        global_weights = federated_averaging (client_weights)\n",
    "        \n",
    "        global_model.load_state_dict (global_weights)\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153bed6e",
   "metadata": {},
   "source": [
    "### Experimental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d760001",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c870f050",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading clinic_0\n",
      "loading clinic_1\n",
      "loading clinic_2\n",
      "loading clinic_3\n"
     ]
    }
   ],
   "source": [
    "#Setting up dataset for training the FL model\n",
    "num_clients = 4\n",
    "\n",
    "num_rounds = 3\n",
    "\n",
    "train_loader_0, val_loader_0 = LocalData('clinic_0', range(8050, 8250)).dataloader() #replace range() for testing, and remove when the code is ready for the final run.\n",
    "\n",
    "train_loader_1, val_loader_1 = LocalData('clinic_1', range(8050, 8250)).dataloader() #replace range() for testing, and remove when the code is ready for the final run.\n",
    "\n",
    "train_loader_2, val_loader_2 = LocalData('clinic_2', range(8050, 8250)).dataloader() #replace range() for testing, and remove when the code is ready for the final run.\n",
    "\n",
    "train_loader_3, val_loader_3 = LocalData('clinic_3', range(8050, 8250)).dataloader() #replace range() for testing, and remove when the code is ready for the final run.\n",
    "\n",
    "train_loaders = [train_loader_0, train_loader_1, train_loader_2, train_loader_3]\n",
    "val_loaders = [val_loader_0, val_loader_1, val_loader_2, val_loader_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e504d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def metrics (ground_truths, predictions):\n",
    "    accuracy = accuracy_score(ground_truths, predictions).round(4)\n",
    "    precision = precision_score (ground_truths, predictions).round(4)\n",
    "    recall = recall_score (ground_truths, predictions).round(4)\n",
    "    f1 = f1_score (ground_truths, predictions).round(4)\n",
    "    confusion_ma = confusion_matrix (ground_truths, predictions)\n",
    "    \n",
    "    print ('Accuracy score: ', accuracy)\n",
    "    print ('Precision score: ', precision)\n",
    "    print ('Recall score: ', recall)\n",
    "    print ('F1 score: ', f1)\n",
    "    print ('Confusion Matrix: \\n', confusion_ma)\n",
    "    return accuracy, precision, recall, f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c23f57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train client model on clinic4's train_loader\n",
    "#Make prediction on clinic4's val_loader\n",
    "def evaluation(client_model):\n",
    "    client_model.eval()\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_clinic5:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = client_model(images)\n",
    "            output = output.round()\n",
    "            predictions.append (output)\n",
    "            ground_truths.append (labels)\n",
    "    predictions = np.concatenate (predictions).reshape(-1).astype ('int')\n",
    "    ground_truths = np.concatenate (ground_truths)\n",
    "    \n",
    "    return metrics (ground_truths, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c08fd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "client_models = [custom_resnet18, resnet18, vgg16, vgg19]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e429c85f",
   "metadata": {},
   "source": [
    "#### 1. Evaluation on the held-out clinic\n",
    "After several rounds of training, the global model's weights are now used as initiallized weights for a fresh client model. Then, we will use this model to make prediction on the eval dataset on the 5th clinic's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5adec8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading clinic_4\n"
     ]
    }
   ],
   "source": [
    "#Setting up dataset for evaluation on clinic 5\n",
    "train_loader_clinic5, val_loader_clinic5 = LocalData ('clinic_4', range (7550, 7750)).dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87d93401",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "train_loss: 0.6997802495956421, train_accuracy: 0.5062500238418579\n",
      "Validation Loss: 0.6944, Validation Accuracy: 0.5000\n",
      "Epoch 2/10:\n",
      "train_loss: 0.6935171961784363, train_accuracy: 0.5625\n",
      "Validation Loss: 0.7037, Validation Accuracy: 0.4844\n",
      "Epoch 3/10:\n",
      "train_loss: 0.698354184627533, train_accuracy: 0.518750011920929\n",
      "Validation Loss: 0.7323, Validation Accuracy: 0.3281\n",
      "Epoch 4/10:\n",
      "train_loss: 0.6860613107681275, train_accuracy: 0.550000011920929\n",
      "Validation Loss: 0.7489, Validation Accuracy: 0.3125\n",
      "Epoch 5/10:\n",
      "train_loss: 0.6839393973350525, train_accuracy: 0.59375\n",
      "Validation Loss: 0.7605, Validation Accuracy: 0.2969\n",
      "Epoch 6/10:\n",
      "train_loss: 0.7038799405097962, train_accuracy: 0.53125\n",
      "Validation Loss: 0.7619, Validation Accuracy: 0.3125\n",
      "Epoch 7/10:\n",
      "train_loss: 0.7138656973838806, train_accuracy: 0.4312500059604645\n",
      "Validation Loss: 0.7438, Validation Accuracy: 0.3438\n",
      "Epoch 8/10:\n",
      "train_loss: 0.692143440246582, train_accuracy: 0.4937500059604645\n",
      "Validation Loss: 0.7277, Validation Accuracy: 0.3906\n",
      "Epoch 9/10:\n",
      "train_loss: 0.6886550068855286, train_accuracy: 0.518750011920929\n",
      "Validation Loss: 0.7133, Validation Accuracy: 0.4062\n",
      "Epoch 10/10:\n",
      "train_loss: 0.6852011442184448, train_accuracy: 0.543749988079071\n",
      "Validation Loss: 0.7073, Validation Accuracy: 0.4219\n",
      "Accuracy score:  0.525\n",
      "Precision score:  0.4074\n",
      "Recall score:  0.7857\n",
      "F1 score:  0.5366\n",
      "Confusion Matrix: \n",
      " [[10 16]\n",
      " [ 3 11]]\n",
      "Epoch 1/10:\n",
      "train_loss: 0.7515281558036804, train_accuracy: 0.46875\n",
      "Validation Loss: 0.7070, Validation Accuracy: 0.3281\n",
      "Epoch 2/10:\n",
      "train_loss: 0.710110890865326, train_accuracy: 0.5249999761581421\n",
      "Validation Loss: 0.6817, Validation Accuracy: 0.4375\n",
      "Epoch 3/10:\n",
      "train_loss: 0.6272143602371216, train_accuracy: 0.65625\n",
      "Validation Loss: 0.6487, Validation Accuracy: 0.6094\n",
      "Epoch 4/10:\n",
      "train_loss: 0.6012279152870178, train_accuracy: 0.731249988079071\n",
      "Validation Loss: 0.6156, Validation Accuracy: 0.7344\n",
      "Epoch 5/10:\n",
      "train_loss: 0.531634384393692, train_accuracy: 0.75\n",
      "Validation Loss: 0.5839, Validation Accuracy: 0.7656\n",
      "Epoch 6/10:\n",
      "train_loss: 0.49145816564559935, train_accuracy: 0.8125\n",
      "Validation Loss: 0.5353, Validation Accuracy: 0.7656\n",
      "Epoch 7/10:\n",
      "train_loss: 0.49594701528549195, train_accuracy: 0.831250011920929\n",
      "Validation Loss: 0.5138, Validation Accuracy: 0.8594\n",
      "Epoch 8/10:\n",
      "train_loss: 0.4632518351078033, train_accuracy: 0.8500000238418579\n",
      "Validation Loss: 0.4956, Validation Accuracy: 0.8750\n",
      "Epoch 9/10:\n",
      "train_loss: 0.4447323799133301, train_accuracy: 0.84375\n",
      "Validation Loss: 0.4752, Validation Accuracy: 0.8750\n",
      "Epoch 10/10:\n",
      "train_loss: 0.4397252380847931, train_accuracy: 0.8500000238418579\n",
      "Validation Loss: 0.4860, Validation Accuracy: 0.8750\n",
      "Accuracy score:  0.875\n",
      "Precision score:  0.7647\n",
      "Recall score:  0.9286\n",
      "F1 score:  0.8387\n",
      "Confusion Matrix: \n",
      " [[22  4]\n",
      " [ 1 13]]\n",
      "Epoch 1/10:\n",
      "train_loss: 0.6653386235237122, train_accuracy: 0.5687500238418579\n",
      "Validation Loss: 0.6039, Validation Accuracy: 0.8125\n",
      "Epoch 2/10:\n",
      "train_loss: 0.5418014407157898, train_accuracy: 0.7875000238418579\n",
      "Validation Loss: 0.6197, Validation Accuracy: 0.6562\n",
      "Epoch 3/10:\n",
      "train_loss: 0.40186561942100524, train_accuracy: 0.8187500238418579\n",
      "Validation Loss: 0.5145, Validation Accuracy: 0.7969\n",
      "Epoch 4/10:\n",
      "train_loss: 0.2950016662478447, train_accuracy: 0.887499988079071\n",
      "Validation Loss: 0.6087, Validation Accuracy: 0.6406\n",
      "Epoch 5/10:\n",
      "train_loss: 0.19224909394979478, train_accuracy: 0.9375\n",
      "Validation Loss: 0.4992, Validation Accuracy: 0.8594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client_model \u001b[38;5;129;01min\u001b[39;00m client_models:\n\u001b[1;32m      2\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m client_model()\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     client_model \u001b[38;5;241m=\u001b[39m train_local_model (client_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEFAULT\u001b[39m\u001b[38;5;124m'\u001b[39m), train_loader_clinic5, val_loader_clinic5)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m     accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m evaluation (client_model)\n\u001b[1;32m      5\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend ([model_name, accuracy, precision, recall, f1])\n",
      "Cell \u001b[0;32mIn[49], line 28\u001b[0m, in \u001b[0;36mtrain_local_model\u001b[0;34m(model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchvision/models/vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[1;32m     67\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for client_model in client_models:\n",
    "    model_name = client_model().__class__.__name__\n",
    "    client_model = train_local_model (client_model('DEFAULT'), train_loader_clinic5, val_loader_clinic5)[0]\n",
    "    accuracy, precision, recall, f1 = evaluation (client_model)\n",
    "    results.append ([model_name, accuracy, precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for client_model in client_models:\n",
    "    global_model = federated_learning (client_model, num_clients, 2, train_loaders, val_loaders) #change num_rounds according to your GPU, which mine doesn't have one :<.\n",
    "    model = train_local_model (global_model, train_loader_clinic5, val_loader_clinic5)[0]\n",
    "    accuracy, precision, recall, f1 = evaluation (model)\n",
    "    model_name = f'FedAVG {client_model().__class__.__name__}'\n",
    "    results.append ([model_name, accuracy, precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b19feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame (columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1'],\n",
    "                       data = results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a95d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b3973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee45139",
   "metadata": {},
   "source": [
    "#### 2. Evaluation on each client's validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a286da",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for client_model in client_models:\n",
    "    model_name = client_model().__class__.__name__\n",
    "    for i in range (4):\n",
    "        local_model = train_local_model (client_model('DEFAULT'), train_loaders[i], val_loaders[i])[0]\n",
    "        accuracy, precision, recall, f1 = evaluation (local_model)\n",
    "        results_2.append ([model_name, f'Clinic_{i}', accuracy, precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3937612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for client_model in client_models:\n",
    "    global_model = federated_learning (client_model, num_clients, 2, train_loaders, val_loaders) #change num_rounds according to your GPU, which mine doesn't have one :<.\n",
    "    for i in range (4):\n",
    "        model = train_local_model (global_model, train_loaders[i], val_loaders[i])[0]\n",
    "        accuracy, precision, recall, f1 = evaluation (model)\n",
    "        model_name = f'FedAVG {client_model().__class__.__name__}'\n",
    "        results_2.append ([model_name, f'Clinic_{i}', accuracy, precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae690de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_2 = pd.DataFrame (columns = ['Model', 'Clinic', 'Accuracy', 'Precision', 'Recall', 'F1'],\n",
    "                         data = results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ff738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_2.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
